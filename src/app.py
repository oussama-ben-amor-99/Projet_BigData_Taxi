import streamlit as st
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
from prometheus_client import start_http_server, Gauge, REGISTRY
import os

# --- 1. CONFIGURATION PROMETHEUS (MONITORING) ---
# @st.cache_resource emp√™che de relancer cette fonction √† chaque clic
@st.cache_resource
def init_monitoring():
    # A. D√©marrer le serveur HTTP (Port 8000)
    try:
        start_http_server(8000)
    except OSError:
        # Si le port est occup√©, c'est que le serveur tourne d√©j√†. On continue.
        pass

    # B. Le "Nettoyeur" (Pour √©viter l'erreur Duplicated timeseries)
    metric_name = 'taxi_last_price_dollars'
    
    # Si la m√©trique existe d√©j√† dans le registre global (m√©moire Python), on la supprime
    if metric_name in REGISTRY._names_to_collectors:
        try:
            REGISTRY.unregister(REGISTRY._names_to_collectors[metric_name])
        except KeyError:
            pass # S√©curit√© suppl√©mentaire

    # C. Cr√©ation de la jauge toute neuve
    return Gauge(metric_name, 'Dernier prix estim√© par le mod√®le')

# On initialise la jauge une bonne fois pour toutes
PRICE_GAUGE = init_monitoring()


# --- 2. CONFIGURATION SPARK ---
# On cr√©e la session Spark (n√©cessaire pour cr√©er le DataFrame d'entr√©e)
spark = SparkSession.builder \
    .appName("Taxi_App") \
    .master("local[*]") \
    .getOrCreate()


# --- 3. CHARGEMENT DU PIPELINE ---
@st.cache_resource
def load_pipeline():
    # On pointe vers le dossier du PIPELINE (pas juste le mod√®le)
    model_path = "/app/data/pipeline_final"
    
    if os.path.exists(model_path):
        # On utilise PipelineModel car on a sauvegard√© un pipeline
        return PipelineModel.load(model_path)
    else:
        return None

model = load_pipeline()


# --- 4. INTERFACE STREAMLIT ---
st.title("üöñ NYC Taxi Price Predictor")
st.markdown("""
Cette application utilise un **Pipeline Spark** (VectorAssembler + LinearRegression) 
pour estimer le prix et envoie les donn√©es √† **Grafana** en temps r√©el.
""")

# Formulaire de saisie
col1, col2 = st.columns(2)
with col1:
    distance = st.slider("Distance (miles)", 0.5, 100.0, 2.0, step=0.5)
with col2:
    hour = st.slider("Heure de la journ√©e", 0, 23, 14)

day = st.selectbox("Jour de la semaine", [1, 2, 3, 4, 5, 6, 7], 
                   format_func=lambda x: ["Dimanche", "Lundi", "Mardi", "Mercredi", "Jeudi", "Vendredi", "Samedi"][x-1])

# Bouton de calcul
if st.button("Calculer le prix üöÄ"):
    if model:
        try:
            # 1. Cr√©ation des donn√©es brutes
            # Spark a besoin d'une liste de tuples
            input_data = spark.createDataFrame(
                [(float(distance), int(hour), int(day))],
                ["trip_distance", "hour", "day_of_week"]
            )
            
            # 2. Pr√©diction via le Pipeline
            # Le pipeline va automatiquement vectoriser les colonnes gr√¢ce au VectorAssembler int√©gr√©
            prediction = model.transform(input_data)
            
            # 3. Extraction du r√©sultat
            price = prediction.select("prediction").first()[0]
            
            # 4. Affichage Streamlit
            st.success(f"üí∞ Prix estim√© : **${price:.2f}**")
            
            # 5. Envoi √† Prometheus/Grafana
            PRICE_GAUGE.set(price)
            st.caption(f"‚úÖ La valeur {price:.2f} a √©t√© envoy√©e au monitoring.")
            
        except Exception as e:
            st.error(f"Erreur lors du calcul : {e}")
    else:
        st.error("‚ö†Ô∏è Pipeline introuvable. As-tu bien lanc√© 'python src/train_model.py' ?")